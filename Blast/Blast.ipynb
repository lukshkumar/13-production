{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukshkumar\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importing all dependencies\n",
    "\n",
    "# Data Manipulation - Pandas, Numpy, Math\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "import json\n",
    "import requests\n",
    "#from Functions_TOS import * \n",
    "\n",
    "\n",
    "# Web Scraping Packages - Selenium, Beautiful, Request\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver #Externally Downloaded\n",
    "from selenium.webdriver.common.keys import Keys #Externally Downloaded\n",
    "from selenium.webdriver.chrome.options import Options #Externally Downloaded\n",
    "\n",
    "#HTML Parser \n",
    "#from html_table_parser.parser import HTMLTableParser #INFO: We don't really need this for now as it's only for cosmetic view of table in Jupyter Notebooks.\n",
    "from lxml import html\n",
    "#import pandas_datareader.data as web\n",
    "\n",
    "# Stocks Data Pulling \n",
    "#from pystocktwits import * #Error: Need Github Repo. Externally Downloaded\n",
    "#from waybackpy import Cdx # Error: Can't include Cdx\n",
    "#from waybackpy import * #Externally Downloaded\n",
    "\n",
    "# Date, Time and Calendar Packages\n",
    "\n",
    "from datetime import date, timedelta, datetime, time\n",
    "import time\n",
    "import datetime\n",
    "#import pandas_market_calendars as mcal #Externally Downloaded\n",
    "from calendar import monthrange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcustom_global_re_indexing(data,number):\n",
    "\t\"\"\"Adds new index as a multiple of a given number \n",
    "\tArgs: \n",
    "\tdata : what data to add index to \n",
    "\tnumber : which number's multiples to use\"\"\"\n",
    "\tnumbers = len(data)\n",
    "\tnumbers_ = []\n",
    "\tfor  i in range(1,numbers+1):\n",
    "\t\tnumbers_.append(i*number)\n",
    "\tdata['Global_Index'] = numbers_\n",
    "\treturn data \n",
    "\n",
    "def get_date_from_global_re_index(data,global_re_indexing_value):\n",
    "\t\"\"\"Gets Date from global index. If exact match not found round to next upper number  . \n",
    "\te.g if we are using the multiples of 3 \n",
    "\tand we enter 5 it will return the date at index of 6\n",
    "\tArgs: \n",
    "\tdata : what data to use\n",
    "\tglobal_re_indexing_value : what index to look for .\"\"\"\n",
    "\tdf_ = data[data['Global_Index'] == global_re_indexing_value][\"Date\"]\n",
    "\tif len(df_) == 0 : \n",
    "\t\tworking_numbers = list(data['Global_Index'])\n",
    "\t\tgreter_than = []\n",
    "\t\tfor _ in working_numbers : \n",
    "\t\t\tif _ > global_re_indexing_value: \n",
    "\t\t\t\tgreter_than.append(_)\n",
    "\t\tgreter_than =sorted(greter_than)\n",
    "\t\tdf_ = data[data[\"Global_Index\"] == greter_than[0]][\"Date\"].tolist()[0]\n",
    "\treturn df_ \n",
    "\n",
    "def SimpleMovingAvg(data, price, length):\n",
    "\t\"\"\"Gets simple moving average for the given data\n",
    "\tArgumnets : \n",
    "\tdata : data to work with \n",
    "\tprice : column \n",
    "\tlength : how many days to work for \"\"\"\n",
    "\tsymbols = data[\"Symbol\"].unique()\n",
    "\tsma = []\n",
    "\tfor _ in symbols: \n",
    "\t\tfor __ in data[data['Symbol'] == _][price].rolling(length).mean():\n",
    "\t\t\tsma.append(__)\n",
    "\treturn sma\n",
    "\n",
    "def get_highest_column_value(data,columns,offset):\n",
    "\t\"\"\"Returns a column with highest values for that given row\n",
    "\tdata: data to work with \n",
    "\tcolumns :What columns to work with\n",
    "\toffset : Wether to get second highest,third highest....\"\"\"\n",
    "\tdf = data[columns]\n",
    "\tdataFRAME  = df.values.tolist()\n",
    "\tnew_column = []\n",
    "\tfor row in dataFRAME : \n",
    "\t\tnew_column.append(sorted(row,reverse = True)[offset])\n",
    "\treturn new_column\n",
    "\n",
    "def get_lowest_column_value(data,columns,offset):\n",
    "\t\"\"\"Returns a column with lowest values for that given row\n",
    "\tdata: data to work with \n",
    "\tcolumns :What columns to work with\n",
    "\toffset : Wether to get second lowest,third lowest....\"\"\"\n",
    "\tdf = data[columns]\n",
    "\tdataFRAME  = df.values.tolist()\n",
    "\tnew_column = []\n",
    "\tfor row in dataFRAME : \n",
    "\t\tnew_column.append(sorted(row)[offset])\n",
    "\treturn new_column\n",
    "\n",
    "def get_sequence(data,columns,strings):\n",
    "\t\"\"\"It returns a column based on  sorted values in descending order by matching with given letters\n",
    "\tdata:data to work with \n",
    "\tcolumns : what columns to work with \n",
    "\t strings : what strings to assign to the column \"\"\"\n",
    "\tdf = data[columns]\n",
    "\tsequences  = []\n",
    "\tfor row in df.iterrows():\n",
    "\t\t\tlist_ = row[1].to_list()\n",
    "\t\t\tres = {strings[i]: list_[i] for i in range(len(strings))}\n",
    "\t\t\tres = {k: v for k, v in sorted(res.items(), key=lambda item: item[1],reverse = True)}\n",
    "\t\t\tsequences.append(\"\".join(res.keys()))\n",
    "\treturn sequences\n",
    "\n",
    "def get_permutation_value(data,column):\n",
    "\t\"\"\"Returns a new column with permuation order of data\n",
    "\tdata : data to work with \n",
    "\tcolumns : columns to work with \"\"\"\n",
    "\tdata = data[column]\n",
    "\ts = data.iloc[0]\n",
    "\tnums = list(s)\n",
    "\tpermutations = {\"\".join(element): index for index,element in enumerate(list(itertools.permutations(nums)))}  \n",
    "\tpermutations_ = []\n",
    "\tfor value in  data: \n",
    "\t\tpermutations_.append(permutations[value])\n",
    "\treturn permutations_\n",
    "\n",
    "def get_custom_symbol_parsed_re_indexing(data,number):\n",
    "\t\"\"\"Adds custom index for each symbol by given number's multiples \n",
    "\tArgs:\n",
    "\tdata : what data to add index to \n",
    "\tnumber: which number's multiple to add as new index \"\"\"\n",
    "\tsymbols = data[\"Symbol\"].unique()\n",
    "\tnumbers = []\n",
    "\tfor _  in symbols: \n",
    "\t\tdf_ = data[data['Symbol'] == _]\n",
    "\t\tfor  i in range(1,len(df_)+1):\n",
    "\t\t\tnumbers.append(i*number)\n",
    "\tdata['Symbol_re_indexing'] = numbers\n",
    "\treturn data\n",
    "\n",
    "def get_date_from_custom_symbolparsed_re_indexing(data,symbol,number):\n",
    "\t\"\"\"Gets date from custom symbol index : \n",
    "\tArgs : \n",
    "\tdata :what dat to look in \n",
    "\tsymbol : what symbol to look for \n",
    "\tnumber :what index to get date from \"\"\"\n",
    "\tsymbol = symbol.upper()\n",
    "\tdata = data[data['Symbol'] == symbol]\n",
    "\tdf_ = data[data['Symbol_re_indexing']== number][\"Date\"]\n",
    "\tif len(df_) == 0 : \n",
    "\t\tworking_numbers = list(data['Symbol_re_indexing'])\n",
    "\t\tgreter_than = []\n",
    "\t\tfor _ in working_numbers : \n",
    "\t\t\tif _ > number: \n",
    "\t\t\t\tgreter_than.append(_)\n",
    "\t\tgreter_than =sorted(greter_than)\n",
    "\t\tdf_ = data[data[\"Symbol_re_indexing\"] == greter_than[0]][\"Date\"].tolist()[0]\n",
    "\treturn df_\n",
    "\n",
    "def get_date_from_re_indexing(data,starting_number, ending_number):\n",
    "    \"\"\"\n",
    "    starting_number = starting_number + 5 - (starting_number %  5)\n",
    "    ending_number = ending_number + 5 - (ending_number %  5)\n",
    "    print(starting_number)\n",
    "    print(ending_number)\n",
    "    df_ = data[data['Symbol_re_indexing']== starting_number]\n",
    "    df_end = data[data['Symbol_re_indexing']== ending_number]\n",
    "    \"\"\"\n",
    "    df_ = data.iloc[[starting_number], :]\n",
    "    df_ = df_.append(data.iloc[[ending_number], :])\n",
    "    if 'Time' in df_.columns:\n",
    "        df_ = df_[[\"Date\", \"Time\"]]\n",
    "    else:\n",
    "        df_ = df_[\"Date\"]\n",
    "    \n",
    "    if len(df_) == 0 : \n",
    "        working_numbers = list(data['Symbol_re_indexing'])\n",
    "        greter_than = []\n",
    "        for _ in working_numbers : \n",
    "            if _ > number: \n",
    "                greter_than.append(_)\n",
    "        greter_than =sorted(greter_than)\n",
    "        df_ = data[data[\"Symbol_re_indexing\"] == greter_than[0]][\"Date\"].tolist()[0]\n",
    "    return df_\n",
    "\n",
    "def get_data_from_re_indexing(data,starting_number, ending_number):\n",
    "\tdf_ = data[(data['Symbol_re_indexing'] >= starting_number) & (data['Symbol_re_indexing'] <= ending_number)]\n",
    "\tif len(df_) == 0 : \n",
    "\t\tworking_numbers = list(data['Symbol_re_indexing'])\n",
    "\t\tgreter_than = []\n",
    "\t\tfor _ in working_numbers : \n",
    "\t\t\tif _ > number: \n",
    "\t\t\t\tgreter_than.append(_)\n",
    "\t\tgreter_than =sorted(greter_than)\n",
    "\t\tdf_ = data[data[\"Symbol_re_indexing\"] == greter_than[0]][\"Date\"].tolist()[0]\n",
    "\treturn df_\n",
    "\n",
    "def get_start_end_only_data(data,start_date, end_date=None):\n",
    "    \"\"\"Gets all data between two given dates \n",
    "    Args: \n",
    "    data :What data to perform the calculations on \n",
    "    start_date : The start date for the data \n",
    "    end_date : the end date for the data\"\"\"\n",
    "    #Date =list(data.Date)\n",
    "    #date = []\n",
    "    #for _ in Date: \n",
    "    #    print(_)\n",
    "    #\t_ = str(_)\n",
    "    #\tdate.append(f'{_[:4]}-{_[4:6]}-{_[6:]}')\n",
    "    #print(date)\n",
    "    \n",
    "    data.Date  = pd.to_datetime(data.Date)\n",
    "    if end_date :\n",
    "        mask = (data['Date'] >= start_date) & (data['Date'] <= end_date)\n",
    "    else: \n",
    "        mask = (data['Date'] >= start_date)\n",
    "    return data.loc[mask]\n",
    "\n",
    "def getquery_blast_txt_file (data,symbol,column,start_date,end_date):\n",
    "\t\"\"\" Makes a txt file  by concating all values for  a given columm between given indexes\n",
    "\tdata: data to work with \n",
    "\tsymbol : symbol to get data for \n",
    "\tcolumn : column to get data for \n",
    "\tstart_date : start date\n",
    "\tend_date : end date \"\"\"\n",
    "\tdata  = get_start_end_only_data(data,start_date,end_date)\n",
    "\ttotal_rows_in_query_dataframe  = data.shape[0]    \n",
    "\tstart = 0 \n",
    "\tend = len(data)-1\n",
    "\tfile = open(f\"Blast/query_{symbol}.txt\",\"w\")\n",
    "\tfile.write(f\">query_{symbol}\\n\")\n",
    "\tdata= list(data[data['Symbol'] == symbol][column])\n",
    "\tdata  = data[start:end]    \n",
    "\tdata_ = []\n",
    "\tfor _ in data : \n",
    "\t\tdata_.append(str(_))\n",
    "\tdata = \"\".join(data_)\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\treturn total_rows_in_query_dataframe\n",
    "\n",
    "    \n",
    "def getsubject_blast_txt_file (data,symbol,column,start_date,end_date, query_start_date, query_end_date):\n",
    "    \"\"\" Makes a txt file  by concating all values for  a given columm between given indexes\n",
    "    data: data to work with \n",
    "    symbol : symbol to get data for \n",
    "    column : column to get data for \n",
    "    start_date : start date\n",
    "    end_date : end date \"\"\"\n",
    "    data  = get_start_end_only_data(data,start_date,end_date)    \n",
    "    \n",
    "    # Making sure that we don't get the query data within subject itself because we don't want the same match of query data\n",
    "    # with itself. Therefore removing the query data from this subject.\n",
    "    \n",
    "    data_after_query = data.copy(deep=True)\n",
    "    mask_before_query = (data['Date'] < query_start_date)\n",
    "    mask_after_query = (data['Date'] > query_end_date)\n",
    "    data = data.loc[mask_before_query]\n",
    "    data_after_query = data_after_query.loc[mask_after_query]\n",
    "    data = data.append(data_after_query)\n",
    "    data = data.reset_index(drop = True)\n",
    "    \n",
    "    subject_starting_date = str(data.loc[0, \"Date\"])\n",
    "    subject_ending_date = str(data.loc[len(data.index) - 1, \"Date\"])\n",
    "    \n",
    "    print(\"Trimmed Subject Starting Date: \", subject_starting_date)\n",
    "    print(\"Trimmed Subject Ending Date: \", subject_ending_date)\n",
    "    \n",
    "    start = 0 \n",
    "    end = len(data)-1\n",
    "    file = open(f\"Blast/subject_{symbol}.txt\",\"w\")\n",
    "    file.write(f\">subject_{symbol}\\n\")\n",
    "    data= list(data[data['Symbol'] == symbol][column])\n",
    "    data  = data[start:end]\n",
    "    data_ = []\n",
    "    for _ in data : \n",
    "        data_.append(str(_))\n",
    "    data = \"\".join(data_)\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def SimpleMovingAvg(data, price, length):\n",
    "\t\"\"\"Gets simple moving average for the given data\n",
    "\tArgumnets : \n",
    "\tdata : data to work with \n",
    "\tprice : column \n",
    "\tlength : how many days to work for \"\"\"\n",
    "\tsymbols = data[\"Symbol\"].unique()\n",
    "\tsma = []\n",
    "\tfor _ in symbols: \n",
    "\t\tfor __ in data[data['Symbol'] == _][price].rolling(length).mean():\n",
    "\t\t\tsma.append(__)\n",
    "\treturn sma\n",
    "\n",
    "def Variance(data, price, length):\n",
    "\t\"\"\"Gets Variacne for the given data\n",
    "\tArgumnets : \n",
    "\tdata : data to work with \n",
    "\tprice : column \n",
    "\tlength : how many days to work for \"\"\"\n",
    "\tsymbols = data[\"Symbol\"].unique()\n",
    "\tsma = []\n",
    "\tfor _ in symbols: \n",
    "\t\tfor __ in data[data['Symbol'] == _][price].rolling(length).var():\n",
    "\t\t\tif __ == 0 : \n",
    "\t\t\t\tsma.append(1)\n",
    "\t\t\telse :\n",
    "\t\t\t\tsma.append(__)\n",
    "\treturn sma\n",
    "\n",
    "def Start_Range(data,column):\n",
    "\t\"\"\"Gets some math stuff\n",
    "\tArguments: \n",
    "\tdata :What data to get math stuff for \n",
    "\tcolumn : what column to get the math stuff for \"\"\"\n",
    "\tamount = 2\n",
    "\tvalues = [None]\n",
    "\tfor step in range( 1,len(data)): \n",
    "\t\tvalues.append(data['2_day_average'].iloc[step] - math.sqrt( abs(data[\"Variance\"].iloc[step]  - (data[\"2_day_average\"].iloc[step] - data[column].iloc[step-1] )**2  )) )\t\t\n",
    "\treturn values\n",
    "\n",
    "def Start_Range_Recursion(data,column):\n",
    "\t\"\"\"Gets some math stuff\n",
    "\tArguments: \n",
    "\tdata :What data to get math stuff for \n",
    "\tcolumn : what column to get the math stuff for \"\"\"\n",
    "\tfor step in range( 1,len(data)): \n",
    "\t\tvalues=data['2_day_moving_average_recursion'].iloc[step] - math.sqrt(abs(   data[\"Variance_from\"].iloc[step]  - (data[\"2_day_moving_average_recursion\"].iloc[step] - data[column].iloc[step-1] )**2 ) )\n",
    "\treturn values\n",
    "\n",
    "def End_Range(data,column):\n",
    "\t\"\"\"Gets some math stuff\n",
    "\tArguments: \n",
    "\tdata :What data to get math stuff for \n",
    "\tcolumn : what column to get the math stuff for \"\"\"\n",
    "\tamount = 2\n",
    "\tvalues = [None]\n",
    "\tfor step in range( 1,len(data)): \n",
    "\t\tvalues.append(math.sqrt(abs( data['Variance'].iloc[step]  -( data[\"2_day_average\"].iloc[step]  -data[column].iloc[step-1])**2 )) +data[\"2_day_average\"].iloc[step])\t\t\n",
    "\treturn values\n",
    "\n",
    "def End_Range_Recursion(data,column):\n",
    "\t\"\"\"Gets some math stuff\n",
    "\tArguments: \n",
    "\tdata :What data to get math stuff for \n",
    "\tcolumn : what column to get the math stuff for \"\"\"\n",
    "\tamount = 2\n",
    "\tfor step in range( 1,len(data)): \n",
    "\t\tvalues= math.sqrt(abs (data['Variance_from'].iloc[step]  -( data[\"2_day_moving_average_recursion\"].iloc[step]  -data[column].iloc[step-1])**2 )) +data[\"2_day_moving_average_recursion\"].iloc[step]\n",
    "\treturn values\n",
    "\n",
    "def Factor_Fraction(data,column):\n",
    "\t\"\"\"Gets some math stuff\n",
    "\tArguments: \n",
    "\tdata :What data to get math stuff for \n",
    "\tcolumn : what column to get the math stuff for \"\"\"\n",
    "\tvalues = [None]\n",
    "\tfor step in range( 1,len(data)): \n",
    "\t\tif abs(data['Close'].iloc[step] - data['Start_Range'].iloc[step]) < abs(data['Close'].iloc[step] - data['End_Range'].iloc[step]):\n",
    "\t\t\tvalues.append(0)\n",
    "\t\telse: \n",
    "\t\t\tvalues.append(1)\n",
    "\treturn values\n",
    "\n",
    "def fetchMarketOpenDays():\n",
    "    r = requests.get(\"https://financialmodelingprep.com/api/v3/is-the-market-open?apikey=aa8b4631b2ad1b2704741d1a2d6a2611\")\n",
    "\n",
    "    last_year_records = r.json()[\"stockMarketHolidays\"][-1]\n",
    "    last_year_records.pop(\"year\")\n",
    "\n",
    "    holidays_list = []\n",
    "    for i in last_year_records:\n",
    "           holidays_list.append(datetime.datetime.strptime(last_year_records[i], '%Y-%m-%d'))\n",
    "\n",
    "    return holidays_list\n",
    "\n",
    "def ticks(dt):\n",
    "    return (dt - datetime.datetime(1, 1, 1)).total_seconds()\n",
    "    \n",
    "def get_how_to_append_end_data (data_file_from, column_from, index_start_from, index_end_from, data_file_to, index_start_to, append_type ):\n",
    "    \"\"\"\" Does fancy math stuff to help predict something \n",
    "    Arguments : \n",
    "    data_file_from : data file from where we take teh da ta\n",
    "    column_from : column from whree we take the data\n",
    "    index_strart_from : index of the start for the dat_file_from file \n",
    "    index_end_from : index of the end for the dat_file_from file \n",
    "    data_file_to : data to append the data to \n",
    "    index_start_to : index of the data file to append the data to \n",
    "    append_type : wether to use new predicted value for the append or the other \n",
    "    \"\"\"\n",
    "    #print(\"Index Start From: \", index_start_from)\n",
    "    #print(\"Index End From: \", index_end_from)\n",
    "    #print(\"Data File From Shape: \", data_file_from.shape)\n",
    "    \n",
    "    #print(\"-----------------------------Data File From----------------------------------\")\n",
    "    \n",
    "    data_file_from =data_file_from.iloc[index_start_from-1: index_end_from]\n",
    "    #print(data_file_from)\n",
    "    start_date  = data_file_from[\"Date\"].iloc[0]\n",
    "    end_date  = data_file_from[\"Date\"].iloc[-1]\n",
    "    data_file_from['2_day_average'] = SimpleMovingAvg(data_file_from,column_from,2)\n",
    "    data_file_from['Variance'] = Variance(data_file_from,column_from,2)\n",
    "    data_file_from[\"Start_Range\"] = Start_Range(data_file_from ,column_from)\n",
    "    data_file_from[\"End_Range\"] = End_Range(data_file_from ,column_from)\n",
    "    data_file_from[\"Factor_Fraction\"] = Factor_Fraction(data_file_from ,column_from)\n",
    "    data_file_from.drop(index=data_file_from.index[0], \n",
    "            axis=0, \n",
    "            inplace=True)\n",
    "    data_file_to = data_file_to.iloc[index_start_to-1:]\n",
    "    #print(\"-----------------------------Data File To----------------------------------\")\n",
    "    #print(data_file_to)\n",
    "    len_data_files_to = len(data_file_to)\n",
    "    close_recursion = [None]*len(data_file_to)    \n",
    "    close_recursion[-2] = data_file_to[column_from].iloc[-2]\n",
    "    data_file_to[f\"{column_from}_Recurrsion\"]  = close_recursion\n",
    "    varaiance = [] \n",
    "    for x in data_file_from[\"Variance\"]:\n",
    "        varaiance.append(x)\n",
    "    factor_Fraction = [] \n",
    "    for x in data_file_from[\"Factor_Fraction\"]:\n",
    "        factor_Fraction.append(x)\n",
    "    data___ = pd.DataFrame({\"Factor_Fraction_from\":factor_Fraction,\"Variance_from\":varaiance})\n",
    "    data_file_to['2_day_average'] =SimpleMovingAvg(data_file_to,column_from,2)\n",
    "    moving_average_recursion = [None]*len(data_file_to)\n",
    "    moving_average_recursion[-1] = data_file_to[\"2_day_average\"].iloc[-1]\n",
    "    data_file_to[\"2_day_moving_average_recursion\"]  = moving_average_recursion\n",
    "    #print(data_file_to)    \n",
    "    data_file_to = pd.concat([data_file_to,pd.DataFrame(data___)],ignore_index=False)\n",
    "    #print(data_file_to)\n",
    "    data_file_to['Variance_from'] = data_file_to['Variance_from'].shift(-1)\n",
    "    data_file_to['Factor_Fraction_from'] = data_file_to['Factor_Fraction_from'].shift(-1)\n",
    "    data_file_to = data_file_to[[column_from,\"Date\",'Factor_Fraction_from',\"Variance_from\",\"2_day_moving_average_recursion\",\"Symbol\",f\"{column_from}_Recurrsion\"]]\n",
    "    data_file_to[\"Start_Range\"] = [None]*len(data_file_to)\n",
    "    data_file_to[\"End_Range\"] = [None]*len(data_file_to)\n",
    "    data_file_to = data_file_to.dropna(how = \"all\")\n",
    "    #print(\"-----------------------------Data File To----------------------------------\")\n",
    "    #print(data_file_to)\n",
    "    #data_file_from.to_csv('Test-Append.csv')\n",
    "    for working_index in range(len_data_files_to-1,len(data_file_to)):\n",
    "            #print(data_file_to)\n",
    "            values  = Start_Range_Recursion(data_file_to.iloc[working_index-1:working_index+1],f\"{column_from}_Recurrsion\")\n",
    "            idx = data_file_to.iloc[working_index].name\n",
    "            data_file_to.at[idx,\"Start_Range\"] = values\n",
    "            values  = End_Range_Recursion(data_file_to.iloc[working_index-1:working_index+1],f\"{column_from}_Recurrsion\")\n",
    "            idx = data_file_to.iloc[working_index].name\n",
    "            data_file_to.at[idx,\"End_Range\"] = values\n",
    "            if  data_file_to.iloc[working_index][\"Factor_Fraction_from\"] == 0: \n",
    "                data_file_to.at[data_file_to.iloc[working_index].name,f\"{column_from}_Recurrsion\"] = data_file_to.iloc[working_index][\"Start_Range\"]\n",
    "            else : \n",
    "                data_file_to.at[data_file_to.iloc[working_index].name,f\"{column_from}_Recurrsion\"] = data_file_to.iloc[working_index][\"End_Range\"]\n",
    "            moving_average = data_file_to.iloc[working_index-1:working_index+1][f\"{column_from}_Recurrsion\"].mean()\n",
    "            try: \n",
    "                data_file_to.at[data_file_to.iloc[working_index+1].name,\"2_day_moving_average_recursion\"] =moving_average\n",
    "            except:\n",
    "                pass\n",
    "    data_file_to = data_file_to[[column_from,f'{column_from}_Recurrsion',\"Symbol\",\"Date\"]]\n",
    "    Dates_to_work_with = list(data_file_to.dropna(subset = [\"Date\"]).Date)\n",
    "    number_of_misisng_dates = data_file_to.Date.isna().sum()\n",
    "    Symbols_to_work_with = list(data_file_to.dropna(subset = [\"Symbol\"]).Symbol)\n",
    "    close_recurstion_data = list(data_file_to.dropna(subset=[f\"{column_from}_Recurrsion\"])[f\"{column_from}_Recurrsion\"])\n",
    "    close_data = list(data_file_to.dropna(subset = [column_from])[column_from])\n",
    "    close_recursion_data = close_recurstion_data[2:]\n",
    "    close_data.extend(close_recursion_data)\n",
    "    #print(\"-----------------------------Data File To----------------------------------\")\n",
    "    #print(data_file_to)\n",
    "    \n",
    "    for number in range(number_of_misisng_dates):\n",
    "        try:\n",
    "            date_time_obj = Dates_to_work_with[-1] + datetime.timedelta(1)\n",
    "        except:\n",
    "            date_time_obj = Dates_to_work_with[-1] + datetime.timedelta(1)          \n",
    "        Dates_to_work_with.append(date_time_obj)\n",
    "        Symbols_to_work_with.append(Symbols_to_work_with[-1])\n",
    "\n",
    "    data_file_to[\"Date\"] = Dates_to_work_with\n",
    "    data_file_to[\"Symbol\"] = Symbols_to_work_with\n",
    "    data_file_to[column_from] = close_data\n",
    "    del data_file_to[f\"{column_from}_Recurrsion\"]\n",
    "    #print(data_file_to.to_csv(\"Test.csv\"))\n",
    "    \n",
    "    # Checking the Authenticated dates:\n",
    "    start_of_trading_day = data_file_to.loc[0,\"Date\"]\n",
    "    end_of_trading_day = data_file_to.iloc[-1][\"Date\"]\n",
    "    start_of_trading_day = datetime.datetime.strftime(start_of_trading_day, '%Y-%m-%d')\n",
    "    start_of_trading_day = datetime.datetime.strptime(start_of_trading_day, '%Y-%m-%d')\n",
    "    \n",
    "    #print(start_of_trading_day)\n",
    "    #print(type(start_of_trading_day))\n",
    "    day_delta = datetime.timedelta(days=1)\n",
    "    dates_of_market_open = []\n",
    "    i = 0\n",
    "    holidays_list = fetchMarketOpenDays()\n",
    "    while(len(dates_of_market_open) != data_file_to.shape[0]):\n",
    "        current_date = start_of_trading_day + i*day_delta\n",
    "        if(((current_date not in holidays_list)) and (current_date.weekday() < 5)):\n",
    "            dates_of_market_open.append(current_date)\n",
    "        i += 1\n",
    "    data_file_to[\"Date\"] = Dates_to_work_with\n",
    "    #print(dates_of_market_open)\n",
    "        \n",
    "    #print(some_error)\n",
    "    return data_file_to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blast - Daily Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_starting_date = \"20220417\"\n",
    "query_ending_date = \"20220517\"\n",
    "\n",
    "take_entire_data_for_subject = True\n",
    "\n",
    "#Only specify starting and ending dates if you are not taking the entire data in subject.\n",
    "subject_starting_date = \"19900102\"\n",
    "subject_ending_date = \"20220503\"\n",
    "\n",
    "# We have 250 rows in query and we want 1/10th of the query in predicted so we put 10 in ratio_of_predicted_entries. \n",
    "# The Predicted data will be 1/10th of the size of the query data. This 10 here is the value of denominator.\n",
    "ratio_of_predicted_entries = 1\n",
    "\n",
    "# Number of top results to match from blast.\n",
    "top_n_blast_results = 10\n",
    "\n",
    "# Run for all tickers within a directory\n",
    "run_for_all_tickers_within_a_directory = False\n",
    "\n",
    "# If run_for_all_tickers_within_a_directory = false then mention the list of tickers you want to run the program for.\n",
    "data_files = [\"SPX\"]\n",
    "\n",
    "#If running on Live system in real-time then set it to false. This will make unique files and won't override one on another.\n",
    "dev_testing = True\n",
    "\n",
    "#List of Moving Averages\n",
    "\n",
    "list_of_MA_list = [\n",
    "    [5, 20, 50, 100, 200],\n",
    "    [9, 19, 41, 88, 129],\n",
    "    [2, 3, 4, 6, 13]\n",
    "    \n",
    "] \n",
    "\n",
    "# list_of_MA_list = [\n",
    "#     [5, 20, 50, 100, 200]\n",
    "#     ] \n",
    "\n",
    "# [10, 18, 52, 71, 75]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Data Blast Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blast Ticker Received From File:  ['AAPL', 'TSLA', 'AMZN', 'GOOG', 'NVDA', 'MSFT']\n",
      "---------------- Filename: SPX.csv ----------------\n",
      "Subject Start Date:  19280103\n",
      "Subject Ending Date:  20220517\n",
      "Total Rows in Query:  22\n",
      "Predicted Rows:  22\n",
      "Trimmed Subject Starting Date:  1928-01-03 00:00:00\n",
      "Trimmed Subject Ending Date:  2022-04-14 00:00:00\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:02:56\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_SPX.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0066148 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:09\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_AAPL.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0021951 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:16\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_TSLA.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0021624 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:20\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_AMZN.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0029034 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:26\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_GOOG.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0014157 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:32\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_NVDA.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0022812 seconds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/12/2022 02:03:38\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_MSFT.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0082742 seconds.\n",
      "\n",
      "\n",
      "I:  0\n",
      "Match is Found in:  SPX     Ratio:  80.0\n",
      "Date Start:  19320317\n",
      "Date End:  19320418\n",
      "-----------------------------------------------------------------------------\n",
      "I:  1\n",
      "Match is Found in:  TSLA     Ratio:  80.0\n",
      "Date Start:  20160105\n",
      "Date End:  20160202\n",
      "-----------------------------------------------------------------------------\n",
      "I:  2\n",
      "Match is Found in:  TSLA     Ratio:  80.0\n",
      "Date Start:  20161026\n",
      "Date End:  20161118\n",
      "-----------------------------------------------------------------------------\n",
      "I:  3\n",
      "Match is Found in:  AAPL     Ratio:  79.0\n",
      "Date Start:  19910708\n",
      "Date End:  19910802\n",
      "-----------------------------------------------------------------------------\n",
      "I:  4\n",
      "Match is Found in:  TSLA     Ratio:  78.0\n",
      "Date Start:  20160105\n",
      "Date End:  20160204\n",
      "-----------------------------------------------------------------------------\n",
      "I:  5\n",
      "Match is Found in:  AAPL     Ratio:  78.0\n",
      "Date Start:  20160701\n",
      "Date End:  20160801\n",
      "-----------------------------------------------------------------------------\n",
      "I:  6\n",
      "Match is Found in:  AAPL     Ratio:  78.0\n",
      "Date Start:  20020819\n",
      "Date End:  20020917\n",
      "-----------------------------------------------------------------------------\n",
      "I:  7\n",
      "Match is Found in:  GOOG     Ratio:  78.0\n",
      "Date Start:  20141128\n",
      "Date End:  20141222\n",
      "-----------------------------------------------------------------------------\n",
      "I:  8\n",
      "Match is Found in:  NVDA     Ratio:  77.0\n",
      "Date Start:  20181204\n",
      "Date End:  20181228\n",
      "-----------------------------------------------------------------------------\n",
      "I:  9\n",
      "Match is Found in:  SPX     Ratio:  77.0\n",
      "Date Start:  19810820\n",
      "Date End:  19810914\n",
      "-----------------------------------------------------------------------------\n",
      "---------------- Program Execution Completed Successfully ----------------\n"
     ]
    }
   ],
   "source": [
    "# Number of letters in re-indexed columns\n",
    "number_of_letters = 0\n",
    "for each_MA_list in list_of_MA_list:\n",
    "    number_of_letters += len(each_MA_list) \n",
    "\n",
    "blast_results_metric = \"Identity\"\n",
    "datetime_ticks = \"\"\n",
    "if(dev_testing):\n",
    "    datetime_ticks = \"-\" + str(ticks(datetime.datetime.utcnow()))\n",
    "list_of_MA_list_string = str(list_of_MA_list)\n",
    "blast_index_start = 100\n",
    "blast_index_end = 200\n",
    "\n",
    "folder_path = \"Tickers/\"\n",
    "blast_folder_path = \"Blast/\"\n",
    "\n",
    "if(run_for_all_tickers_within_a_directory):\n",
    "    data_files = os.listdir(folder_path)\n",
    "else:\n",
    "    for each_file_index_in_directory in range(len(data_files)):\n",
    "        data_files[each_file_index_in_directory] = data_files[each_file_index_in_directory] + \".csv\"\n",
    "\n",
    "subject_files = os.listdir(blast_folder_path)\n",
    "#Reading the Blast-Tickers File\n",
    "\n",
    "blast_tickers = pd.read_csv(\"Blast-Tickers.csv\")\n",
    "blast_tickers = blast_tickers[\"Tickers\"].values.tolist()\n",
    "print(\"Blast Ticker Received From File: \", blast_tickers)\n",
    "\n",
    "for data_file in data_files:\n",
    "\n",
    "    print(\"---------------- Filename: \" + data_file + \" ----------------\")\n",
    "    symbol_name = data_file.replace(\".csv\",\"\")\n",
    "    df_daily_data = pd.read_csv(folder_path + data_file)\n",
    "    df_subject_data_to_merge_with_prediction_data = df_daily_data.copy(deep=True)\n",
    "    \n",
    "    if(take_entire_data_for_subject):\n",
    "        subject_starting_date = str(df_daily_data.loc[0, \"Date\"])\n",
    "        subject_ending_date = str(df_daily_data.loc[len(df_daily_data.index) - 1, \"Date\"])\n",
    "        print(\"Subject Start Date: \", subject_starting_date)\n",
    "        print(\"Subject Ending Date: \", subject_ending_date)\n",
    "\n",
    "    #Making a separate folder for each ticker.\n",
    "    if(not os.path.isdir(blast_folder_path + \"Tickers/\" + symbol_name)):\n",
    "        os.mkdir(blast_folder_path + \"Tickers/\" + symbol_name)\n",
    "    blast_resultant_filename = blast_folder_path + \"Tickers/\" + symbol_name + \"/results_query_\" + symbol_name\n",
    "    current_subject_filename = \"subject_\" + symbol_name + \".txt\"\n",
    "    current_query_filename = \"query_\" + symbol_name + \".txt\"\n",
    "\n",
    "    df_daily_data = df_daily_data.astype({\"Date\": str})\n",
    "\n",
    "    getcustom_global_re_indexing(df_daily_data,number_of_letters)\n",
    "\n",
    "    MA_list_count = 1\n",
    "    for each_MA_list in list_of_MA_list:\n",
    "\n",
    "        list_of_each_MA_value_column_name = []\n",
    "        each_MA_list_sequence_name = \"sequence\" + str(MA_list_count)\n",
    "        for each_MA_value in each_MA_list:\n",
    "\n",
    "            each_MA_value_column_name = str(each_MA_value) + \"_day_sma\"\n",
    "            list_of_each_MA_value_column_name.append(each_MA_value_column_name)\n",
    "            #SMA Daily\n",
    "            df_daily_data[each_MA_value_column_name] = SimpleMovingAvg(df_daily_data, \"Close\", each_MA_value)\n",
    "\n",
    "        df_daily_data[each_MA_list_sequence_name] = get_sequence(df_daily_data,list_of_each_MA_value_column_name,[\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "        MA_list_count += 1\n",
    "\n",
    "    get_custom_symbol_parsed_re_indexing(df_daily_data,number_of_letters)\n",
    "\n",
    "    # Merging all list of MA columns into one single column named as sequence\n",
    "    consolidated_sequence = []\n",
    "    for i in range(len(df_daily_data)):\n",
    "        each_row_sequence = \"\"\n",
    "        for j in range(1, len(list_of_MA_list)+1):\n",
    "            j_column_name = \"sequence\" + str(j)\n",
    "            each_row_sequence += df_daily_data.loc[i, j_column_name]\n",
    "        consolidated_sequence.append(each_row_sequence)\n",
    "\n",
    "    df_daily_data[\"sequence\"] = consolidated_sequence\n",
    "\n",
    "    #df_daily_data.to_csv(\"Testing-\" + symbol_name +\".csv\")\n",
    "\n",
    "    #getquery_blast_txt_file (data,symbol,column,start_date,end_date)\n",
    "    total_rows_in_query_dataframe = getquery_blast_txt_file (df_daily_data,symbol_name,\"sequence\",query_starting_date, query_ending_date)\n",
    "    print(\"Total Rows in Query: \", total_rows_in_query_dataframe)\n",
    "    number_of_predicted_entries = int(total_rows_in_query_dataframe / ratio_of_predicted_entries)\n",
    "    print(\"Predicted Rows: \", number_of_predicted_entries)\n",
    "\n",
    "    #getsubject_blast_txt_file (data,symbol,column,start_date,end_date)\n",
    "    getsubject_blast_txt_file (df_daily_data,symbol_name,\"sequence\",subject_starting_date, subject_ending_date, query_starting_date, query_ending_date)\n",
    "\n",
    "    #BLAST INTEGRATION - FOR CURRENT SYMBOL\n",
    "    !makeblastdb -in C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_subject_filename -dbtype prot -out C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
    "    !blastp -query C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_query_filename -db C:\\Users\\lukshkumar\\Desktop\\Blast\\DB -out C:\\Users\\lukshkumar\\Desktop\\Blast\\output.txt -outfmt 6 -max_hsps $top_n_blast_results\n",
    "\n",
    "    blast_result_csv_file = pd.DataFrame(columns = [\"Ticker\", \"Query\", \"Subject\", \"Identity\", \"Alignment Length\", \"MisMatches\", \"Gap Opens\", \"Query Start\", \"Query End\", \"Subject Start\", \"Subject End\", \"Evalue\", \"Bit Score\"])\n",
    "    file = open(blast_folder_path  + \"output.txt\",\"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    for line in lines:\n",
    "        blast_file_result = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        blast_file_result.insert(0,symbol_name)\n",
    "        blast_result_csv_file.loc[len(blast_result_csv_file.index)] = blast_file_result\n",
    "\n",
    "    #BLAST INTEGRATION - FOR OTHER SYMBOLS IN THE LIST\n",
    "    for blast_ticker in blast_tickers:\n",
    "        if(blast_ticker != symbol_name):\n",
    "            blast_subject_filename = \"subject_\" + blast_ticker + \".txt\"\n",
    "\n",
    "            if(os.path.isfile(blast_folder_path +  blast_subject_filename)):\n",
    "                # MAKE BLAST DB FROM THE SUBJECT\n",
    "                !makeblastdb -in C:\\Users\\lukshkumar\\Desktop\\Blast\\$blast_subject_filename -dbtype prot -out C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
    "                # RUN BLAST TO FIND THE MATCH\n",
    "                !blastp -query C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_query_filename -db C:\\Users\\lukshkumar\\Desktop\\Blast\\DB -out C:\\Users\\lukshkumar\\Desktop\\Blast\\output.txt -outfmt 6 -max_hsps $top_n_blast_results\n",
    "                # APPEND DATA IN CSV FILE\n",
    "                file = open(blast_folder_path  + \"output.txt\",\"r\")\n",
    "                lines = file.readlines()\n",
    "                file.close()\n",
    "                for line in lines:\n",
    "                    blast_file_result = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "                    blast_file_result.insert(0,blast_ticker)\n",
    "                    blast_result_csv_file.loc[len(blast_result_csv_file.index)] = blast_file_result\n",
    "\n",
    "\n",
    "    blast_result_csv_file[blast_results_metric] = pd.to_numeric(blast_result_csv_file[blast_results_metric])\n",
    "    blast_result_csv_file = blast_result_csv_file.sort_values(by=[blast_results_metric], ascending = False).reset_index(drop=True)\n",
    "    blast_result_csv_file.to_csv(blast_resultant_filename + \".csv\", index = False)\n",
    "\n",
    "    #Creating another column for the filename for easier reference.\n",
    "    blast_result_csv_file[\"filename\"] = \"\"\n",
    "    #Making sure that the top_n_blast_results are greater than the number of results in blasts file in case blast file has \n",
    "    #One result only and top results are two then it should not break.\n",
    "    if(top_n_blast_results > blast_result_csv_file.shape[0]):\n",
    "        top_n_blast_results = blast_result_csv_file.shape[0]\n",
    "    for i in range(top_n_blast_results):\n",
    "        print(\"I: \", i)\n",
    "        blast_ticker_name = blast_result_csv_file.iloc[i,0]\n",
    "        blast_identity_percentage = round(blast_result_csv_file.iloc[i,3], 0)\n",
    "        blast_index_start = int(blast_result_csv_file.iloc[i,9])\n",
    "        blast_index_end = int(blast_result_csv_file.iloc[i,10])\n",
    "\n",
    "        #print(\"Blast Index Start: \", blast_index_start)\n",
    "\n",
    "        source_dataframe  = pd.read_csv(folder_path + blast_ticker_name + \".csv\")\n",
    "        source_dataframe_total_rows = source_dataframe.shape[0]\n",
    "        # Appending the data from Matched Ticker to the Target Ticker Data File.\n",
    "        target_dataframe = get_how_to_append_end_data(source_dataframe,\"Close\",math.ceil(blast_index_end/number_of_letters) + 1,math.ceil(blast_index_end/number_of_letters) + number_of_predicted_entries + 2,df_daily_data,df_daily_data.shape[0] - 1,True)\n",
    "\n",
    "        # Adding additional fields in target dataframe (predicted data). \n",
    "        target_dataframe[\"Open\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"High\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"Low\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"Volume\"] = 0\n",
    "        target_dataframe[\"Date\"] = target_dataframe[\"Date\"].dt.strftime('%Y%m%d')\n",
    "\n",
    "        #Dropping the first two unnecessary records from the target dataframe that were only used to create predicted data.\n",
    "        target_dataframe = target_dataframe.iloc[2:,:]\n",
    "\n",
    "        consolidated_filename_date_portion = str(target_dataframe.loc[0, \"Date\"])\n",
    "        # Appending the source data into target dataframe in order to make up a combinded file having previous and predicted data.\n",
    "        consolidated_source_plus_target_df = df_subject_data_to_merge_with_prediction_data.append(target_dataframe)\n",
    "        consolidated_source_plus_target_df.reset_index(drop = True)\n",
    "\n",
    "        # Date Against the Index Matched from Blast\n",
    "        df_date_we_match_found = get_date_from_re_indexing(source_dataframe,(math.ceil(blast_index_start/number_of_letters) -1), (math.ceil(blast_index_end/number_of_letters)))\n",
    "        \n",
    "        consolidated_source_plus_target_filename = symbol_name + consolidated_filename_date_portion + \"D\" + str(ratio_of_predicted_entries) + \"V\" + str(i + 1) + datetime_ticks + \".csv\"\n",
    "        consolidated_source_plus_target_filename_with_path = blast_folder_path + \"Tickers/\" + symbol_name + \"/\" + consolidated_source_plus_target_filename\n",
    "\n",
    "        consolidated_source_plus_target_df.to_csv(consolidated_source_plus_target_filename_with_path, index = False)\n",
    "\n",
    "        print(\"Match is Found in: \", blast_ticker_name, \"    Ratio: \", blast_identity_percentage)\n",
    "        print(\"Date Start: \", df_date_we_match_found.iloc[0])\n",
    "        print(\"Date End: \", df_date_we_match_found.iloc[1])\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "        #Adding new columns into result_query csv file\n",
    "        blast_result_csv_file.loc[i, \"filename\"] =  consolidated_source_plus_target_filename\n",
    "        blast_result_csv_file.loc[i, \"match start date\"] =  df_date_we_match_found.iloc[0]\n",
    "        blast_result_csv_file.loc[i, \"match end date\"] =  df_date_we_match_found.iloc[1]\n",
    "        blast_result_csv_file.loc[i, \"subject size\"] =  source_dataframe_total_rows\n",
    "        blast_result_csv_file.loc[i, \"match settings\"] =  list_of_MA_list_string\n",
    "        blast_result_csv_file.loc[i, \"query size\"] =  total_rows_in_query_dataframe\n",
    "        blast_result_csv_file.loc[i, \"prediction ratio\"] =  '\"1/' + str(ratio_of_predicted_entries) + '\"'\n",
    "        \n",
    "#Saving the updated file with Date added in the filename so that we have a separate blast result file for each day.\n",
    "blast_result_csv_file.to_csv(blast_resultant_filename + \"_\" + consolidated_filename_date_portion + \"_D\" + datetime_ticks + \".csv\", index = False)\n",
    "#Removing the generic file created above to make sure we don't have duplicate data.\n",
    "os.remove(blast_resultant_filename + \".csv\")\n",
    "\n",
    "print(\"---------------- Program Execution Completed Successfully ----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minute Data Blast Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blast Ticker Received From File:  []\n",
      "---------------- Filename: AAPL.csv ----------------\n",
      "Subject Start Date:  20220331\n",
      "Subject Ending Date:  20220517\n",
      "Total Rows in Query:  4609\n",
      "Predicted Rows:  460\n",
      "Trimmed Subject Starting Date:  2022-03-31 00:00:00\n",
      "Trimmed Subject Ending Date:  2022-04-30 00:00:00\n",
      "\n",
      "\n",
      "Building a new DB, current time: 06/05/2022 04:31:49\n",
      "New DB name:   C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "New DB title:  C:\\Users\\lukshkumar\\Desktop\\Blast\\subject_AAPL.txt\n",
      "Sequence type: Protein\n",
      "Deleted existing Protein BLAST database named C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 1 sequences in 0.0019837 seconds.\n",
      "\n",
      "\n",
      "I:  0\n",
      "Match is Found in:  AAPL     Ratio:  47.0\n",
      "Date Start:  20220405\n",
      "Date End:  20220419\n",
      "-----------------------------------------------------------------------------\n",
      "I:  1\n",
      "Match is Found in:  AAPL     Ratio:  45.0\n",
      "Date Start:  20220409\n",
      "Date End:  20220428\n",
      "-----------------------------------------------------------------------------\n",
      "I:  2\n",
      "Match is Found in:  AAPL     Ratio:  45.0\n",
      "Date Start:  20220331\n",
      "Date End:  20220419\n",
      "-----------------------------------------------------------------------------\n",
      "I:  3\n",
      "Match is Found in:  AAPL     Ratio:  45.0\n",
      "Date Start:  20220414\n",
      "Date End:  20220430\n",
      "-----------------------------------------------------------------------------\n",
      "I:  4\n",
      "Match is Found in:  AAPL     Ratio:  45.0\n",
      "Date Start:  20220418\n",
      "Date End:  20220430\n",
      "-----------------------------------------------------------------------------\n",
      "I:  5\n",
      "Match is Found in:  AAPL     Ratio:  45.0\n",
      "Date Start:  20220414\n",
      "Date End:  20220430\n",
      "-----------------------------------------------------------------------------\n",
      "I:  6\n",
      "Match is Found in:  AAPL     Ratio:  43.0\n",
      "Date Start:  20220404\n",
      "Date End:  20220421\n",
      "-----------------------------------------------------------------------------\n",
      "I:  7\n",
      "Match is Found in:  AAPL     Ratio:  43.0\n",
      "Date Start:  20220331\n",
      "Date End:  20220420\n",
      "-----------------------------------------------------------------------------\n",
      "I:  8\n",
      "Match is Found in:  AAPL     Ratio:  43.0\n",
      "Date Start:  20220405\n",
      "Date End:  20220422\n",
      "-----------------------------------------------------------------------------\n",
      "I:  9\n",
      "Match is Found in:  AAPL     Ratio:  43.0\n",
      "Date Start:  20220406\n",
      "Date End:  20220421\n",
      "-----------------------------------------------------------------------------\n",
      "---------------- Program Execution Completed Successfully ----------------\n"
     ]
    }
   ],
   "source": [
    "# Number of letters in re-indexed columns\n",
    "number_of_letters = 0\n",
    "for each_MA_list in list_of_MA_list:\n",
    "    number_of_letters += len(each_MA_list) \n",
    "\n",
    "blast_results_metric = \"Identity\"\n",
    "\n",
    "blast_index_start = 100\n",
    "blast_index_end = 200\n",
    "\n",
    "folder_path = \"Tickers-Minute-Data/\"\n",
    "blast_folder_path = \"Blast/\"\n",
    "\n",
    "if(run_for_all_tickers_within_a_directory):\n",
    "    data_files = os.listdir(folder_path)\n",
    "else:\n",
    "    for each_file_index_in_directory in range(len(data_files)):\n",
    "        data_files[each_file_index_in_directory] = data_files[each_file_index_in_directory] + \".csv\"\n",
    "\n",
    "subject_files = os.listdir(blast_folder_path)\n",
    "#Reading the Blast-Tickers File\n",
    "\n",
    "blast_tickers = pd.read_csv(\"Blast-Tickers.csv\")\n",
    "blast_tickers = blast_tickers[\"Tickers\"].values.tolist()\n",
    "print(\"Blast Ticker Received From File: \", blast_tickers)\n",
    "\n",
    "for data_file in data_files:\n",
    "\n",
    "    print(\"---------------- Filename: \" + data_file + \" ----------------\")\n",
    "    symbol_name = data_file.replace(\".csv\",\"\")\n",
    "    df_daily_data = pd.read_csv(folder_path + data_file)\n",
    "\n",
    "    if(take_entire_data_for_subject):\n",
    "        subject_starting_date = str(df_daily_data.loc[0, \"Date\"])\n",
    "        subject_ending_date = str(df_daily_data.loc[len(df_daily_data.index) - 1, \"Date\"])\n",
    "        print(\"Subject Start Date: \", subject_starting_date)\n",
    "        print(\"Subject Ending Date: \", subject_ending_date)\n",
    "\n",
    "    #Making a separate folder for each ticker.\n",
    "    if(not os.path.isdir(blast_folder_path + \"Tickers/\" + symbol_name)):\n",
    "        os.mkdir(blast_folder_path + \"Tickers/\" + symbol_name)\n",
    "    blast_resultant_filename = blast_folder_path + \"Tickers/\" + symbol_name + \"/results_query_\" + symbol_name\n",
    "    current_subject_filename = \"subject_\" + symbol_name + \".txt\"\n",
    "    current_query_filename = \"query_\" + symbol_name + \".txt\"\n",
    "\n",
    "    df_daily_data = df_daily_data.astype({\"Date\": str})\n",
    "\n",
    "    getcustom_global_re_indexing(df_daily_data,number_of_letters)\n",
    "\n",
    "    MA_list_count = 1\n",
    "    for each_MA_list in list_of_MA_list:\n",
    "\n",
    "        list_of_each_MA_value_column_name = []\n",
    "        each_MA_list_sequence_name = \"sequence\" + str(MA_list_count)\n",
    "        for each_MA_value in each_MA_list:\n",
    "\n",
    "            each_MA_value_column_name = str(each_MA_value) + \"_day_sma\"\n",
    "            list_of_each_MA_value_column_name.append(each_MA_value_column_name)\n",
    "            #SMA Daily\n",
    "            df_daily_data[each_MA_value_column_name] = SimpleMovingAvg(df_daily_data, \"Close\", each_MA_value)\n",
    "\n",
    "        df_daily_data[each_MA_list_sequence_name] = get_sequence(df_daily_data,list_of_each_MA_value_column_name,[\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "        MA_list_count += 1\n",
    "\n",
    "    get_custom_symbol_parsed_re_indexing(df_daily_data,number_of_letters)\n",
    "\n",
    "    # Merging all list of MA columns into one single column named as sequence\n",
    "    consolidated_sequence = []\n",
    "    for i in range(len(df_daily_data)):\n",
    "        each_row_sequence = \"\"\n",
    "        for j in range(1, len(list_of_MA_list)+1):\n",
    "            j_column_name = \"sequence\" + str(j)\n",
    "            each_row_sequence += df_daily_data.loc[i, j_column_name]\n",
    "        consolidated_sequence.append(each_row_sequence)\n",
    "\n",
    "    df_daily_data[\"sequence\"] = consolidated_sequence\n",
    "\n",
    "    #df_daily_data.to_csv(\"Testing-\" + symbol_name +\".csv\")\n",
    "\n",
    "    #getquery_blast_txt_file (data,symbol,column,start_date,end_date)\n",
    "    total_rows_in_query_dataframe = getquery_blast_txt_file (df_daily_data,symbol_name,\"sequence\",query_starting_date, query_ending_date)\n",
    "    print(\"Total Rows in Query: \", total_rows_in_query_dataframe)\n",
    "    number_of_predicted_entries = int(total_rows_in_query_dataframe / ratio_of_predicted_entries)\n",
    "    print(\"Predicted Rows: \", number_of_predicted_entries)\n",
    "\n",
    "    #getsubject_blast_txt_file (data,symbol,column,start_date,end_date)\n",
    "    getsubject_blast_txt_file (df_daily_data,symbol_name,\"sequence\",subject_starting_date, subject_ending_date, query_starting_date, query_ending_date)\n",
    "\n",
    "    #BLAST INTEGRATION - FOR CURRENT SYMBOL\n",
    "    !makeblastdb -in C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_subject_filename -dbtype prot -out C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
    "    !blastp -query C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_query_filename -db C:\\Users\\lukshkumar\\Desktop\\Blast\\DB -out C:\\Users\\lukshkumar\\Desktop\\Blast\\output.txt -outfmt 6 -max_hsps $top_n_blast_results\n",
    "\n",
    "    blast_result_csv_file = pd.DataFrame(columns = [\"Ticker\", \"Query\", \"Subject\", \"Identity\", \"Alignment Length\", \"MisMatches\", \"Gap Opens\", \"Query Start\", \"Query End\", \"Subject Start\", \"Subject End\", \"Evalue\", \"Bit Score\"])\n",
    "    file = open(blast_folder_path  + \"output.txt\",\"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    for line in lines:\n",
    "        blast_file_result = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        blast_file_result.insert(0,symbol_name)\n",
    "        blast_result_csv_file.loc[len(blast_result_csv_file.index)] = blast_file_result\n",
    "\n",
    "    #BLAST INTEGRATION - FOR OTHER SYMBOLS IN THE LIST\n",
    "    for blast_ticker in blast_tickers:\n",
    "        if(blast_ticker != symbol_name):\n",
    "            blast_subject_filename = \"subject_\" + blast_ticker + \".txt\"\n",
    "\n",
    "            if(os.path.isfile(blast_folder_path +  blast_subject_filename)):\n",
    "                # MAKE BLAST DB FROM THE SUBJECT\n",
    "                !makeblastdb -in C:\\Users\\lukshkumar\\Desktop\\Blast\\$blast_subject_filename -dbtype prot -out C:\\Users\\lukshkumar\\Desktop\\Blast\\DB\n",
    "                # RUN BLAST TO FIND THE MATCH\n",
    "                !blastp -query C:\\Users\\lukshkumar\\Desktop\\Blast\\$current_query_filename -db C:\\Users\\lukshkumar\\Desktop\\Blast\\DB -out C:\\Users\\lukshkumar\\Desktop\\Blast\\output.txt -outfmt 6 -max_hsps $top_n_blast_results\n",
    "                # APPEND DATA IN CSV FILE\n",
    "                file = open(blast_folder_path  + \"output.txt\",\"r\")\n",
    "                lines = file.readlines()\n",
    "                file.close()\n",
    "                for line in lines:\n",
    "                    blast_file_result = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "                    blast_file_result.insert(0,blast_ticker)\n",
    "                    blast_result_csv_file.loc[len(blast_result_csv_file.index)] = blast_file_result\n",
    "\n",
    "\n",
    "    blast_result_csv_file[blast_results_metric] = pd.to_numeric(blast_result_csv_file[blast_results_metric])\n",
    "    blast_result_csv_file = blast_result_csv_file.sort_values(by=[blast_results_metric], ascending = False).reset_index(drop=True)\n",
    "    blast_result_csv_file.to_csv(blast_resultant_filename + \".csv\", index = False)\n",
    "\n",
    "    #Creating another column for the filename for easier reference.\n",
    "    blast_result_csv_file[\"filename\"] = \"\"\n",
    "    #Making sure that the top_n_blast_results are greater than the number of results in blasts file in case blast file has \n",
    "    #One result only and top results are two then it should not break.\n",
    "    if(top_n_blast_results > blast_result_csv_file.shape[0]):\n",
    "        top_n_blast_results = blast_result_csv_file.shape[0]\n",
    "    for i in range(top_n_blast_results):\n",
    "        print(\"I: \", i)\n",
    "        blast_ticker_name = blast_result_csv_file.iloc[i,0]\n",
    "        blast_identity_percentage = round(blast_result_csv_file.iloc[i,3], 0)\n",
    "        blast_index_start = int(blast_result_csv_file.iloc[i,9])\n",
    "        blast_index_end = int(blast_result_csv_file.iloc[i,10])\n",
    "\n",
    "        #print(\"Blast Index Start: \", blast_index_start)\n",
    "\n",
    "        source_dataframe  = pd.read_csv(folder_path + blast_ticker_name + \".csv\")\n",
    "        # Appending the data from Matched Ticker to the Target Ticker Data File.\n",
    "        target_dataframe = get_how_to_append_end_data(source_dataframe,\"Close\",math.ceil(blast_index_end/number_of_letters) + 1,math.ceil(blast_index_end/number_of_letters) + number_of_predicted_entries + 2,df_daily_data,df_daily_data.shape[0] - 1,True)\n",
    "\n",
    "        # Adding additional fields in target dataframe (predicted data). \n",
    "        target_dataframe[\"Open\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"High\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"Low\"] = target_dataframe[\"Close\"]\n",
    "        target_dataframe[\"Volume\"] = 0\n",
    "        target_dataframe[\"Date\"] = target_dataframe[\"Date\"].dt.strftime('%Y%m%d')\n",
    "\n",
    "        #Dropping the first two unnecessary records from the target dataframe that were only used to create predicted data.\n",
    "        target_dataframe = target_dataframe.iloc[2:,:]\n",
    "\n",
    "        consolidated_filename_date_portion = str(target_dataframe.loc[0, \"Date\"])\n",
    "        # Appending the source data into target dataframe in order to make up a combinded file having previous and predicted data.\n",
    "        consolidated_source_plus_target_df = source_dataframe.append(target_dataframe)\n",
    "        consolidated_source_plus_target_df.reset_index(drop = True)\n",
    "\n",
    "        # Date Against the Index Matched from Blast\n",
    "        df_date_we_match_found = get_date_from_re_indexing(source_dataframe,(math.ceil(blast_index_start/number_of_letters) -1), (math.ceil(blast_index_end/number_of_letters)))\n",
    "\n",
    "        consolidated_source_plus_target_filename = symbol_name + consolidated_filename_date_portion + \"M\" + str(ratio_of_predicted_entries) + \"V\" + str(i + 1) + \".csv\"\n",
    "        consolidated_source_plus_target_filename_with_path = blast_folder_path + \"Tickers/\" + symbol_name + \"/\" + consolidated_source_plus_target_filename\n",
    "\n",
    "        blast_result_csv_file.loc[i, \"filename\"] =  consolidated_source_plus_target_filename\n",
    "\n",
    "        consolidated_source_plus_target_df.to_csv(consolidated_source_plus_target_filename_with_path, index = False)\n",
    "\n",
    "        print(\"Match is Found in: \", blast_ticker_name, \"    Ratio: \", blast_identity_percentage)\n",
    "        print(\"Date Start: \", df_date_we_match_found.iloc[0])\n",
    "        print(\"Date End: \", df_date_we_match_found.iloc[1])\n",
    "        print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "#Saving the updated file with Date added in the filename so that we have a separate blast result file for each day.\n",
    "blast_result_csv_file.to_csv(blast_resultant_filename + \"_\" + consolidated_filename_date_portion + \"_M\" + \".csv\", index = False)\n",
    "#Removing the generic file created above to make sure we don't have duplicate data.\n",
    "os.remove(blast_resultant_filename + \".csv\")\n",
    "\n",
    "print(\"---------------- Program Execution Completed Successfully ----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
